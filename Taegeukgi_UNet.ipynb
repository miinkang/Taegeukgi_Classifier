{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "koreanflag_unet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fVhFuexnlf94WUjimcLiFa4VhJ2f8BAJ",
      "authorship_tag": "ABX9TyP1DPbynB3k89QpuOfn+EJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miinkang/Taeguekgi_Classifier/blob/main/Taegeukgi_UNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8idwGih-EbK8"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CP25nz-E_ky"
      },
      "source": [
        "use_colab = True\n",
        "assert use_colab in [True, False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA3XFPBPE_6V"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjlDY5KXFAXd"
      },
      "source": [
        "from __future__ import absolute_import, division\n",
        "from __future__ import print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "# import matplotlib as mpl\n",
        "# mpl.rcParams['axes.grid'] = False\n",
        "# mpl.rcParams['figure.figsize'] = (12,12)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import preprocessing\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_W3EgpPJnxa"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train = ImageDataGenerator(rescale = 1/255)\n",
        "val =ImageDataGenerator(rescale=1/255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeyT-TgAIy4X"
      },
      "source": [
        "train_dataset = train.flow_from_directory('/content/drive/MyDrive/[바로알리기공모전]3456/data/train',\n",
        "                                          target_size = (200,200),\n",
        "                                          batch_size = 32,\n",
        "                                          shuffle=True,\n",
        "                                          class_mode='binary')\n",
        "\n",
        "val_dataset = train.flow_from_directory('/content/drive/MyDrive/[바로알리기공모전]3456/data/val',\n",
        "                                          target_size = (200,200),\n",
        "                                          batch_size = 32,\n",
        "                                          shuffle=True,\n",
        "                                          class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVp34oxmQ8X4"
      },
      "source": [
        "# print(train_dataset[0].shape, val_dataset[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ehJoh7uFA6G"
      },
      "source": [
        "class Conv(tf.keras.Model):\n",
        "        def __init__(self, num_filters, kernel_size, input_shape=(200,200,3)):\n",
        "            super(Conv, self).__init__()\n",
        "            self.conv_1 = layers.Conv2D(num_filters, kernel_size, padding='same')# Conv2D\n",
        "            self.bn_1 = layers.BatchNormalization()# batch norm\n",
        "            self.relu = layers.ReLU()\n",
        "            self.dropout_1 = layers.Dropout(0.5)\n",
        "\n",
        "        def call(self, inputs, training=True):\n",
        "            x = self.conv_1(inputs)# conv\n",
        "            x = self.bn_1(x, training=True)# batch norm\n",
        "            x = self.relu(x) # relu\n",
        "            # if training:\n",
        "            #     x = self.dropout_1(x)\n",
        "\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ifvclukFBK-"
      },
      "source": [
        "class ConvBlock(tf.keras.Model):\n",
        "        def __init__(self, num_filters):\n",
        "            super(ConvBlock, self).__init__()\n",
        "            self.conv1 = Conv(num_filters, kernel_size=3) # Conv class\n",
        "            self.conv2 = Conv(num_filters, kernel_size=3) # Conv class\n",
        "\n",
        "        def call(self, inputs, training=True):\n",
        "            encoder = self.conv1(inputs) # conv1\n",
        "            encoder = self.conv2(encoder) # conv2\n",
        "\n",
        "            return encoder\n",
        "    \n",
        "class ConvBlock_R(tf.keras.Model):\n",
        "    def __init__(self, num_filters):\n",
        "        super(ConvBlock_R, self).__init__()\n",
        "        self.conv1 = Conv(num_filters, kernel_size=3)# Conv class \n",
        "        self.conv2 = Conv(num_filters, kernel_size=3)# Conv class\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        decoder = self.conv1(inputs)# conv1\n",
        "        decoder = self.conv2(decoder)# conv2\n",
        "\n",
        "        return decoder\n",
        "\n",
        "\n",
        "class EncoderBlock(tf.keras.Model):\n",
        "    def __init__(self, num_filters):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.conv_block = ConvBlock(num_filters) # Conv block\n",
        "        self.encoder_pool = layers.MaxPooling2D()# max pool\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        encoder = self.conv_block(inputs)# conv block\n",
        "        encoder_pool = self.encoder_pool(encoder)# encoder pool\n",
        "\n",
        "        return encoder_pool, encoder\n",
        "\n",
        "\n",
        "class DecoderBlock(tf.keras.Model):\n",
        "    def __init__(self, num_filters):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.convT = layers.Conv2DTranspose(num_filters, kernel_size=3, strides=2, padding='same') # conv transpose\n",
        "        self.bn = layers.BatchNormalization()# batch norm\n",
        "        self.conv_block_r = ConvBlock_R(num_filters) # convblock R\n",
        "        self.relu = layers.ReLU()\n",
        "        self.dropout_1 = layers.Dropout(0.5)\n",
        "\n",
        "    def call(self, input_tensor, concat_tensor, training=True):\n",
        "        # convT - bn - relu - concat - conv black R\n",
        "        decoder = self.convT(input_tensor) \n",
        "        decoder = self.bn(decoder)\n",
        "        decoder = self.relu(decoder)\n",
        "        # if training:\n",
        "        #     decoder = self.dropout_1(decoder)\n",
        "        decoder = tf.concat([concat_tensor, decoder], -1) \n",
        "        decoder = self.conv_block_r(decoder) \n",
        "        \n",
        "        \n",
        "\n",
        "        return decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEbVFUmrFBbZ"
      },
      "source": [
        "class UNet(tf.keras.Model):\n",
        "        def __init__(self):\n",
        "            super(UNet, self).__init__()\n",
        "            self.encoder_input = tf.keras.layers.Input((200, 200, 3))\n",
        "            self.encoder_block1 = EncoderBlock(32) # encoder 32\n",
        "            self.encoder_block2 = EncoderBlock(64) # encoder 64\n",
        "            self.encoder_block3 = EncoderBlock(128) # encoder 128\n",
        "            self.encoder_block4 = EncoderBlock(256) # encoder 256\n",
        "\n",
        "            self.center = ConvBlock(512) # conv block 512\n",
        "\n",
        "            self.decoder_block4 = DecoderBlock(256) # decoder 256\n",
        "            self.decoder_block3 = DecoderBlock(128) # decoder 128\n",
        "            self.decoder_block2 = DecoderBlock(64) # decoder 64\n",
        "            self.decoder_block1 = DecoderBlock(32) # decoder 32\n",
        "\n",
        "            self.output_conv = layers.Conv2D(1, 1, activation='sigmoid') # a output layer conv2d\n",
        "            # padding='same'이 없으니 kernel size를 1로, 1x1 conv를 사용함. \n",
        "            # 1x1 conv 는 dense 와 같은 역할을 한다. \n",
        "\n",
        "        def call(self, inputs, training=True): \n",
        "            # inputs = self.encoder_input\n",
        "            encoder1_pool, encoder1 = self.encoder_block1(inputs) # encoder1 outputs\n",
        "            encoder2_pool, encoder2 = self.encoder_block2(encoder1_pool) # encoder2 outputs\n",
        "            encoder3_pool, encoder3 = self.encoder_block3(encoder2_pool) # encoder3 outputs\n",
        "            encoder4_pool, encoder4 = self.encoder_block4(encoder3_pool) # encoder4 outputs\n",
        "            # pooling, concat용 데이터 \n",
        "            center = self.center(encoder4_pool) # center outputs\n",
        "\n",
        "            decoder4 = self.decoder_block4(center, encoder4)# decoder4 output\n",
        "            decoder3 = self.decoder_block3(decoder4, encoder3)# decoder3 output\n",
        "            decoder2 = self.decoder_block2(decoder3, encoder2)# decoder2 output\n",
        "            decoder1 = self.decoder_block1(decoder2, encoder1)# decoder1 output\n",
        "\n",
        "            outputs = self.output_conv(decoder1) # the model output\n",
        "\n",
        "            return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RBv2ZldFBqY"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "loss = tf.keras.losses.BinaryCrossentropy\n",
        "max_epochs = 50\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRZvq8JxMSiO"
      },
      "source": [
        "model = UNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwNIgpU3FB6s"
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtqUWBvVFCPb"
      },
      "source": [
        "checkpoint_dir = 'drive/MyDrive'\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 monitor='val_loss',\n",
        "                                                 mode='auto',\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2CSdMQnFCaq"
      },
      "source": [
        "cos_decay = tf.keras.experimental.CosineDecay(0.0001, \n",
        "                                              max_epochs)\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(cos_decay, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZPC5GHRFCGg"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK5bL46pIq0X"
      },
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    epochs=max_epochs,\n",
        "                    steps_per_epoch=1000//batch_size,\n",
        "                    validation_data=val_dataset,\n",
        "                    # validation_steps=num_test_examples//batch_size,\n",
        "                    callbacks=[cp_callback, lr_callback, cos_decay]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jBV0PdsIrDy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}